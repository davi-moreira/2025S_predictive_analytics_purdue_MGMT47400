---
title: "<span style = 'font-size: 100%;'> MGMT 47400: Predictive Analytics </span>"
subtitle: "<span style = 'font-size: 150%;'> Support Vector Machines </span>"
author: "Professor: Davi Moreira"
# date: "2024-08-01"
date-format: "MMMM DD, YYYY"
format:
  revealjs: 
    transition: slide
    background-transition: fade
    width: 1600
    height: 900
    center: true
    slide-number: true
    incremental: true
    chalkboard: 
      buttons: false
    preview-links: auto
    #logo: images/quarto.png
    footer: "Predictive Analytics"
    theme: [simple,custom.scss]
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

## Overview

:::::: nonincremental
::::: columns
::: {.column width="50%" style="text-align: center; justify-content: center; align-items: center;"}
-   XXXX
:::

::: {.column width="50%" style="text-align: center; justify-content: center; align-items: center;"}
-   XXX
:::
:::::
::::::

# XXX {background-color="#cfb991"}

## Support Vector Machines

Here we approach the two-class classification problem in a direct way:

*We try and find a plane that separates the classes in feature space.*

If we cannot, we get creative in two ways:

- We soften what we mean by "separates", and  

- We enrich and enlarge the feature space so that separation is possible.


## What is a Hyperplane?

- A hyperplane in $p$ dimensions is a flat affine subspace of dimension $p - 1$.

- In general, the equation for a hyperplane has the form  

$$
  \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p = 0
$$

- In $p = 2$ dimensions, a hyperplane is a line.

- If $\beta_0 = 0$, the hyperplane goes through the origin; otherwise, it does not.

- The vector $\beta = (\beta_1, \beta_2, \dots, \beta_p)$ is called the normal vector — it points in a direction orthogonal to the surface of the hyperplane.


## Hyperplane in 2 Dimensions

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_1_1-1.png") 
```

## Separating Hyperplanes


```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_2-1.png") 
```

- If $f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$, then $f(X) > 0$ for points on one side of the hyperplane, and $f(X) < 0$ for points on the other.

- If we code the colored points as $Y_i = +1$ for blue and $Y_i = -1$ for mauve, then if $Y_i \cdot f(X_i) >0$ for all $i$, $f(X) = 0$ defines a separating hyperplane.


## Maximal Margin Classifier

Among all separating hyperplanes, find the one that makes the biggest gap or margin between the two classes.

**Constrained optimization problem:**

$$
\text{maximize } M \quad \beta_0, \beta_1, \dots, \beta_p
$$

subject to:

$$
\sum_{j=1}^p \beta_j^2 = 1,
$$
$$
y_i (\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip}) \geq M \quad \text{for all } i = 1, \dots, N.
$$

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_3-1.png") 
```


This can be rephrased as a convex quadratic program and solved efficiently. The function `svm()` in the **e1071** package solves this problem efficiently.


## Non-separable Data

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_4-1.png") 
```

The data on the left are not separable by a linear boundary.

This is often the case, unless $N < p$.


## Noisy Data

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_5-1.png") 
```

Sometimes the data are separable, but noisy. This can lead to a poor solution for the maximal-margin classifier.

The *support vector classifier* maximizes a *soft margin*.


## Support Vector Classifier

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_6-1.png") 
```

$$
\text{maximize } M \quad \beta_0, \beta_1, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n
$$

subject to:

$$
\sum_{j=1}^p \beta_j^2 = 1,
$$

$$
y_i (\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip}) \geq M(1 - \epsilon_i),
$$

$$
\epsilon_i \geq 0, \quad \sum_{i=1}^n \epsilon_i \leq C.
$$


## $C$ is a Regularization Parameter

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_7-1.png") 
```

This layout demonstrates the effect of the regularization parameter $C$ on the SVM classifier with four subplots, each representing a different $C$ value. Adjustments to annotations or layout can be made as needed. Let me know if further modifications are required!


## Linear Boundary Can Fail

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_1_2-1.png") 
```

Sometimes a linear boundary simply won’t work, no matter what value of $C$.

The example on the left is such a case.

What to do?


## Feature Expansion

- Enlarge the space of features by including transformations; e.g. $X_1^2, X_1^3, X_1X_2, X_1X_2^2, \dots$. Hence go from a $p$-dimensional space to a $M > p$-dimensional space.

- Fit a support-vector classifier in the enlarged space.

- This results in non-linear decision boundaries in the original space.

**Example:**  

Suppose we use $(X_1, X_2, X_1^2, X_2^2, X_1X_2)$ instead of just $(X_1, X_2)$. Then the decision boundary would be of the form:

$$
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2 + \beta_5 X_1X_2 = 0
$$

This leads to nonlinear decision boundaries in the original space (quadratic conic sections).


## Cubic Polynomials

Here we use a basis expansion of cubic polynomials:

- From 2 variables to 9.

- The support-vector classifier in the enlarged space solves the problem in the lower-dimensional space.

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_1_3-1.png") 
```

$$
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2 + \beta_5 X_1 X_2 + \beta_6 X_1^3 + \beta_7 X_2^3 + \beta_8 X_1^2 X_2 + \beta_9 X_1 X_2^2 = 0
$$


## Nonlinearities and Kernels

- Polynomials (especially high-dimensional ones) get wild rather fast.

- There is a more elegant and controlled way to introduce nonlinearities in support-vector classifiers — through the use of *kernels*.

- Before we discuss these, we must understand the role of *inner products* in support-vector classifiers.


## Inner Products and Support Vectors

$$
\langle x_i, x_{i'} \rangle = \sum_{j=1}^p x_{ij} x_{i'j} \quad \text{— inner product between vectors}
$$

- The linear support vector classifier can be represented as:

$$
  f(x) = \beta_0 + \sum_{i=1}^n \alpha_i \langle x, x_i \rangle \quad \text{— $n$ parameters}
$$

- To estimate the parameters $\alpha_1, \dots, \alpha_n$ and $\beta_0$, all we need are the $\binom{n}{2}$ inner products $\langle x_i, x_{i'} \rangle$ between all pairs of training observations.

It turns out that most of the $\hat{\alpha}_i$ can be zero:

$$
f(x) = \beta_0 + \sum_{i \in S} \hat{\alpha}_i \langle x, x_i \rangle
$$

$\mathbf{S}$ is the **support set** of indices $i$ such that $\hat{\alpha}_i > 0$. [See slide 8]


## Kernels and Support Vector Machines

- If we can compute inner products between observations, we can fit a SV classifier. Can be quite abstract!

- Some special *kernel functions* can do this for us. E.g.,
$$
  K(x_i, x_{i'}) = \left(1 + \sum_{j=1}^p x_{ij} x_{i'j}\right)^d
$$
  computes the inner products needed for $d$-dimensional polynomials — $\binom{p+d}{d}$ basis functions!  
  *Try it for $p = 2$ and $d = 2$.*

- The solution has the form:
$$
  f(x) = \beta_0 + \sum_{i \in S} \hat{\alpha}_i K(x, x_i).
$$

## Radial Kernel

$$
K(x_i, x_{i'}) = \exp\left(-\gamma \sum_{j=1}^p (x_{ij} - x_{i'j})^2 \right).
$$

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_1_4-1.png") 
```

$$
f(x) = \beta_0 + \sum_{i \in S} \hat{\alpha}_i K(x, x_i).
$$

- Implicit feature space; very high dimensional.  

- Controls variance by squashing down most dimensions severely.

## Example: Heart Data

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_10-1.png") 
```

ROC curve is obtained by changing the threshold $t$ in $\hat{f}(X) > t$, and recording *false positive* and *true positive* rates as $t$ varies. Here we see ROC curves on training data.

## Example Continued: Heart Test Data

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_11-1.png") 
```


## SVMs: More Than 2 Classes?

The SVM as defined works for $K = 2$ classes. What do we do if we have $K > 2$ classes?

**OVA**  

One versus All. Fit $K$ different 2-class SVM classifiers $\hat{f}_k(x)$, $k = 1, \dots, K$; each class versus the rest. Classify $x^*$ to the class for which $\hat{f}_k(x^*)$ is largest.

**OVO**  

One versus One. Fit all $\binom{K}{2}$ pairwise classifiers $\hat{f}_{k\ell}(x)$. Classify $x^*$ to the class that wins the most pairwise competitions.

Which to choose? If $K$ is not too large, use OVO.


## Support Vector versus Logistic Regression?

With $f(X) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p$, we can rephrase support-vector classifier optimization as:

$$
\text{minimize}_{\beta_0, \beta_1, \dots, \beta_p} \left\{ \sum_{i=1}^n \max \big[ 0, 1 - y_i f(x_i) \big] + \lambda \sum_{j=1}^p \beta_j^2 \right\}
$$

- This has the form *loss plus penalty*.  
- The loss is known as the *hinge loss*.  
- Very similar to the *loss* in logistic regression (negative log-likelihood).

```{r  echo=FALSE, out.width = "65%", fig.align="center"}
knitr::include_graphics("figs/9_12-1.png") 
```

## Which to Use: SVM or Logistic Regression

- When classes are (nearly) separable, SVM does better than LR. So does LDA.

- When not, LR (with ridge penalty) and SVM are very similar.

- If you wish to estimate probabilities, LR is the choice.

- For nonlinear boundaries, kernel SVMs are popular. Can use kernels with LR and LDA as well, but computations are more expensive.

## Summary

:::::::: nonincremental
::::::: columns
:::: {.column width="50%"}
::: {style="font-size: 80%;"}
-   XXXX
:::
::::

:::: {.column width="50%"}
::: {style="font-size: 80%;"}
-   XXXX
:::
::::
:::::::
::::::::

# Thank you! {background-color="#cfb991"}
